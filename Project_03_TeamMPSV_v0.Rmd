---
title: "DATA 607 Project 03 v1"
author: "Philip Tanofsky"
date: "3/21/2020"
output: html_document
---

# Introduction

Attempt to determine the top skills for a data scientist based on job listings for data scientist. Different text mining techniques are used to identify skills from the description section of the job listings.


```{r}
# Read in CSV file of the 10000 jobs listings for Data Scientist
jobs_df <- read.csv(file = 'data_scientist_united_states_job_postings_jobspikr.csv', stringsAsFactors = FALSE)

# For testing purposes, just the first 100 rows only
jobs_df <- jobs_df[1:100,]

# Output all the column names to confirm
names(jobs_df)

# Just confirming the class is character for all these fields
#class(jobs_df$job_title)
#class(jobs_df$category)
#class(jobs_df$job_description)
#class(jobs_df$job_type)
#class(jobs_df$job_board)
```

Title is not that valueable as many of the titles are "Data Scientist" with some containing Sr. or I, or some derivative name, so will be ignoring for the analysis.

```{r}
# Following the NASA case study: https://www.tidytextmining.com/nasa.html

# Set up separate tidy data frames for job_title, category, job_description, job_type while keeping the uniq_id for each so we can connect later
library(dplyr)

jobs_title <- tibble(id = jobs_df$uniq_id, title = jobs_df$job_title)

jobs_title %>% print(n=10)
```

Job description is the most important field in the dataset. Contains the complete write-up posted for the job listing.

```{r}
jobs_desc <- tibble(id = jobs_df$uniq_id, 
                        desc = jobs_df$job_description)

jobs_desc %>% 
  select(desc) %>% 
  sample_n(5)
```

Category is interesting info. Used it for the NASA case study, not sure if the results are really telling of anything, but leaving in for now.

```{r}
jobs_cat <- tibble(id = jobs_df$uniq_id, 
                        category = jobs_df$category)

jobs_cat <- jobs_cat %>% filter(jobs_cat$category != "") %>% print(n=100)
```

Type info not valuable, as it's just full-time, contract, etc.

```{r}
jobs_type <- tibble(id = jobs_df$uniq_id, 
                        category = jobs_df$job_type)
jobs_type %>% print(n=10)
```

Attempted to make a list of keywords, which would essentially be the list of skills. Wanted to see if I created this list and then ran it against the job description if there would be a way to count or correlate the two. But since there is no ID as from the dataset, this list has proven not useful. Leaving in for now.

```{r}
keywords <- c("python", "sql", "modeling", "statistics", "algorithms", "r", "visualization", "hadoop", "mining", "communication", "aws", "spark", "artificial intelligence", "machine learning", "sas", "cloud", "innovative", "driven", "optimization", "java", "databases", "leadership", "security", "tableau", "phd", "education", "degree", "hive", "ml", "scala", "ms", "economics", "neural", "verbal", "transformation", "culture", "tensorflow", "automation", "azure", "nlp", "architecture", "nosql", "scripting", "passionate", "agile", "bachelor\'s", "clustering", "pandas", "bs")

jobs_kw <- tibble(keyword = keywords)

jobs_kw
```

From the job_description, tokenize all the words and remove "stop_words" which are common words in the English language to allow for focus on meaningful words of the job listing.

```{r}
# Use tidytextâ€™s unnest_tokens() for the description field so we can do the text analysis.
# unnest_tokens() will tokenize all the words in the description field and create a tidy dataframe of the word by identifer
library(tidytext)

jobs_desc <- jobs_desc %>% 
  unnest_tokens(word, desc) %>% 
  anti_join(stop_words)

jobs_desc
```

Provide count in table form of the most common words in the job descriptions.

```{r}
# Most common words in the description field
jobs_desc %>%
  count(word, sort = TRUE) %>% print(n=10)
```

Added more words to be removed, certainly not exhaustive at this point, but leaving in so can me added to.

```{r}
# added in extra stop words to remove the noise of words in the descriptions
extra_stopwords <- tibble(word = c(as.character(1:10), 
                                    "2", "job", "company", "e.g", "religion", "origin", "color", "gender", "2019", "1999"))
jobs_desc <- jobs_desc %>% 
  anti_join(extra_stopwords)

jobs_desc %>%
  count(word, sort = TRUE) %>% print(n=10)
```

Applied the stemming of words to in essence combine words of the same root, but I'm not sure if it's valuable at this point. Leaving in for now.

```{r}
library(SnowballC)
# Stemming the words, and let's see what top 10 come out as not too useful
# from https://abndistro.com/post/2019/02/10/tidy-text-mining-in-r/#stemming
jobs_desc %>%
  mutate(word_stem = SnowballC::wordStem(word)) %>%
  count(word_stem, sort = TRUE) %>% print(n=10)
# Not very helpful as of yet, definitely stemming words, but then it's too generic
```

Just a list of skills based on the output of the count of common words in the job description after removing the stop_words

Skills: python, sql, modeling, statistics, algorithms, r, visualization, hadoop, mining, communication, aws, spark, artificial intelligence, machine learning, sas, cloud, innovative, driven, optimization, java, databases, leadership, security, tableau, phd, education, degree, hive, ml, scala, ms, economics, neural, verbal, transformation, culture, tensorflow, automation, azure, nlp, architecture, nosql, scripting, passionate, agile, bachelor's, clustering, pandas, bs

Applying lowercase to all the words to ensure different cases aren't problematic

```{r}
# lowercase all the words just to make sure there's no redundancy
jobs_desc <- jobs_desc %>% 
  mutate(word = tolower(word))
```

Count the number of occurrences two words appear together in the description field. This does not mean the two words are next to each other, just that they appear in the same description field.

```{r}
# Word co-ocurrences and correlations
# Count how many times each pair of words occurs together in the description field.
library(widyr)

desc_word_pairs <- jobs_desc %>% 
  pairwise_count(word, id, sort = TRUE, upper = FALSE)

desc_word_pairs
```

Plot network of the co-occuring words.

```{r}
# Plot networks of these co-occuring words
library(ggplot2)
library(igraph)
library(ggraph)

set.seed(1234)
desc_word_pairs %>%
  filter(n >= 50) %>%
  graph_from_data_frame() %>%
  ggraph(layout = "fr") +
  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = "cyan4") +
  geom_node_point(size = 5) +
  geom_node_text(aes(label = name), repel = TRUE, 
                 point.padding = unit(0.2, "lines")) +
  theme_void()
```

Output the correlation of the word pairs.

```{r}
# Find correlation among the words in the description field
# This looks for those words that are more likely to occur together than with other words for a dataset.
desc_cors <- jobs_desc %>% 
  group_by(word) %>%
  filter(n() >= 50) %>%
  pairwise_cor(word, id, sort = TRUE, upper = FALSE)

desc_cors %>% print(n=10)
# Skipping the rest of section 8.2
```

Calculating the term frequency times inverse document frequency.

```{r}
# Calculating tf-idf for the description fields
# we can use tf-idf, the term frequency times inverse document frequency, to identify words that are especially important to a document within a collection of documents.

desc_tf_idf <- jobs_desc %>% 
  count(id, word, sort = TRUE) %>%
  ungroup() %>%
  bind_tf_idf(word, id, n)

desc_tf_idf %>% filter(n >= 10) %>%
  arrange(-tf_idf)

# These are the most important words in the description fields as measured by tf-idf, meaning they are common but not too common.
```

Combining the data frame of the TF_IDF from the job descriptions with the categories. Joining by the unique ID as key.

By the categories, this will identify the most important words from the job descriptions.

```{r fig.width=10,fig.height=11}
#8.3.2
# Try it with the category, not sure if how much value that will be, but want to try it
desc_tf_idf <- full_join(desc_tf_idf, jobs_cat, by = "id")

desc_tf_idf %>% 
  filter(!near(tf, 1)) %>%
  filter(category %in% c("Accounting/Finance", "biotech", 
                        "Computer/Internet", "Arts/Entertainment/Publishing",
                        "military", "business and financial operations",
                        "Engineering/Architecture", "Manufacturing/Mechanical",
                        "life physical and social science", "Banking/Loans",
                        "agriculture and fishing ", "Education/Training",
                        "science")) %>%
  arrange(desc(tf_idf)) %>%
  group_by(category) %>%
  distinct(word, category, .keep_all = TRUE) %>%
  top_n(15, tf_idf) %>% 
  ungroup() %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>%
  ggplot(aes(word, tf_idf, fill = category)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~category, ncol = 3, scales = "free") +
  coord_flip() +
  labs(title = "Highest tf-idf words in DS job listing description fields",
       caption = "From jobpickr dataset",
       x = NULL, y = "tf-idf")
```

For topic modeling, create a document term matrix.

Initially, the word count by document ID.

```{r}
# 8.4 Topic Modeling

# Casting to a document-term matrix
word_counts <- jobs_desc %>%
  count(id, word, sort = TRUE) %>%
  ungroup()

word_counts %>% print(n=10)

```

Now construct the document-term matrix. High level of sparsity. From the case study: "Each non-zero entry corresponds to a certain word appearing in a certain document."Each non-zero entry corresponds to a certain word appearing in a certain document."

```{r}
# Construct DTM
desc_dtm <- word_counts %>%
  cast_dtm(id, word, n)

desc_dtm
```

From wikipedia: In natural language processing, the latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar. 

```{r}
# 8.4.2 Topic modeling
library(topicmodels)

# be aware that running this model is time intensive
# Define there to be 16 topics. I have entered 13 categories, so I figured, 16 is 2^4, and close to 13, so why not.
desc_lda <- LDA(desc_dtm, k = 16, control = list(seed = 1234))
desc_lda
```

Tidy the resulting topics.

```{r}
# Interpreting the data model
tidy_lda <- tidy(desc_lda)

tidy_lda
```

Identify the top 10 terms for each topic.

```{r}
# Top 10 Terms
top_terms <- tidy_lda %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms
```

Plot the top 10 terms for each topic. Interesting to see how the words break out, even though the topics are anonymous, only identified by number.

From case study: The topic modeling process has identified groupings of terms that we can understand as human readers of these description fields.

```{r fig.width=10,fig.height=11}
top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  group_by(topic, term) %>%    
  arrange(desc(beta)) %>%  
  ungroup() %>%
  ggplot(aes(term, beta, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  scale_x_reordered() +
  labs(title = "Top 10 terms in each LDA topic",
       x = NULL, y = expression(beta)) +
  facet_wrap(~ topic, ncol = 4, scales = "free")
```

Now calculate gamma, which will define the probability that each document belongs in each topic

Below graph isn't useful to me.

```{r fig.width=10,fig.height=11}
# LDA gamma
lda_gamma <- tidy(desc_lda, matrix = "gamma")

lda_gamma

# Distribution of probabilities for all topics
ggplot(lda_gamma, aes(gamma)) +
  geom_histogram() +
  scale_y_log10() +
  labs(title = "Distribution of probabilities for all topics",
       y = "Number of documents", x = expression(gamma))
```

Below graph isn't useful to me.

```{r}
# Distribution of probability for each topic
ggplot(lda_gamma, aes(gamma, fill = as.factor(topic))) +
  geom_histogram(show.legend = FALSE) +
  facet_wrap(~ topic, ncol = 4) +
  scale_y_log10() +
  labs(title = "Distribution of probability for each topic",
       y = "Number of documents", x = expression(gamma))
```


discover which categories are associated with which topic.

```{r}
# 8.4.4

lda_gamma <- full_join(lda_gamma, jobs_cat, by = c("document" = "id"))

lda_gamma

top_cats <- lda_gamma %>% 
  filter(gamma > 0.5) %>% 
  count(topic, category, sort = TRUE)

top_cats

```

Little hard to decpiher, but I think connecting this plot to the previous plot of the topics would then connect which words (skills) are meaningful by category.

```{r fig.width=10,fig.height=8}
# One more graph from 8.4.4
top_cats %>%
  group_by(topic) %>%
  top_n(5, n) %>%
  ungroup %>%
  mutate(category = reorder_within(category, n, topic)) %>%
  ggplot(aes(category, n, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  labs(title = "Top categories for each LDA topic",
       x = NULL, y = "Number of documents") +
  coord_flip() +
  scale_x_reordered() +
  facet_wrap(~ topic, ncol = 4, scales = "free")
```

# Another section ... Quanteda attempt

Create the dataframe of the first 100 rows

```{r}
library(tidyverse)
library(quanteda)

# See below for how this works
# https://www.r-bloggers.com/advancing-text-mining-with-r-and-quanteda/

jobs_df <- read.csv(file = 'data_scientist_united_states_job_postings_jobspikr.csv', stringsAsFactors = FALSE)

# For just testing purposes, just the first 100 rows only
jobs_df <- jobs_df[1:100,]

jobs_df <- jobs_df[,c('uniq_id', 'job_description')]

class(jobs_df)
```

Create a corpus based on the unique ID and the job description

```{r}
# Generate a corpus
my_corpus <- corpus(jobs_df, docid_field = "uniq_id", text_field = "job_description")

#my_corpus

mycorpus_stats <- summary(my_corpus)

head(mycorpus_stats)
```

Preprocess the text. Remove numbers, remove punctuation, remove symbols, remove URLs, split hyphens. Because the blog entry included it, I kept the part about cleaning for OCR.

```{r}
# Preprocess the text

# Create tokens
token <-
  tokens(
    my_corpus,
    remove_numbers  = TRUE,
    remove_punct    = TRUE,
    remove_symbols  = TRUE,
    remove_url      = TRUE,
    split_hyphens   = TRUE
  )

# Don't think this is needed by why not
# Clean tokens created by OCR
token_ungd <- tokens_select(
  token,
  c("[\\d-]", "[[:punct:]]", "^.{1,2}$"),
  selection = "remove",
  valuetype = "regex",
  verbose = TRUE
)
```

Create a Data Frequency Matrix, this time using Quanteda

Also, filter words that appear less than 7.5% and more than 90%. This rather conservative approach is possible because we have a sufficiently large corpus.

```{r}
# Data frequency matrix
my_dfm <- dfm(token_ungd,
              tolower = TRUE,
              stem = TRUE,
              remove = stopwords("english")
              )

my_dfm_trim <-
  dfm_trim(
    my_dfm,
    min_docfreq = 0.075,
    # min 7.5%
    max_docfreq = 0.90,
    # max 90%
    docfreq_type = "prop"
  )

head(dfm_sort(my_dfm_trim, decreasing = TRUE, margin = "both"),
     n = 10,
     nf = 10)
```

```{r}
# https://quanteda.io/articles/pkgdown/examples/plotting.html
#corpus_subset(my_corpus) %>%
#    dfm(remove = stopwords("english"), remove_punct = TRUE) %>%
#    dfm_trim(min_termfreq = 1, verbose = FALSE) %>%
#    textplot_wordcloud(comparison = TRUE)
```

```{r fig.width=10,fig.height=11}
textplot_wordcloud(my_dfm, min_count = 10,
     color = c('red', 'pink', 'green', 'purple', 'orange', 'blue'))
```

```{r fig.width=10,fig.height=11}
kwic(my_corpus, pattern = "data") %>%
    textplot_xray()
```

```{r fig.width=10,fig.height=11}
library("ggplot2")

theme_set(theme_bw())

g <- textplot_xray(
     kwic(my_corpus, pattern = "python"),
     kwic(my_corpus, pattern = "r")
)

g + aes(color = keyword) + 
    scale_color_manual(values = c("blue", "red")) +
    theme(legend.position = "none")

```

```{r fig.width=10,fig.height=5}
# Frequency plots
features_dfm <- textstat_frequency(my_dfm, n = 50)

# Sort by reverse frequency order
features_dfm$feature <- with(features_dfm, reorder(feature, -frequency))

ggplot(features_dfm, aes(x = feature, y = frequency)) +
    geom_point() + 
    theme(axis.text.x = element_text(angle = 90, hjust = 1))
```


# Third pass at trying something

From: https://www.tidytextmining.com/

```{r}
library(tidytext)
library(dplyr)
library(ggplot2)
library(tidyr)

jobs_df <- read.csv(file = 'data_scientist_united_states_job_postings_jobspikr.csv', stringsAsFactors = FALSE)

# For just testing purposes, just the first 100 rows only
jobs_df <- jobs_df[1:100,]

jobs_df <- jobs_df[,c('job_description', 'uniq_id')]

jobs_words <- jobs_df %>%
  unnest_tokens(word, job_description) %>%
  anti_join(stop_words) %>%
  count(uniq_id, word, sort = TRUE)

total_words <- jobs_words %>% 
  group_by(uniq_id) %>% 
  summarize(total = sum(n))

jobs_words <- left_join(jobs_words, total_words)

jobs_words
```

Don't see much value in this one. Probably just remove.

```{r fig.width=10,fig.height=11}
ggplot(jobs_words %>% filter(n > 20), aes(n / total, fill = uniq_id)) +
  geom_histogram(show.legend = FALSE) +
  xlim(0.0, 0.4) +
  facet_wrap(~uniq_id, ncol = 2, scales = "free_y")

```

Don't see any value in this one. Probably just remove.

```{r fig.width=10,fig.height=11}
jobs_words <- jobs_words %>%
  bind_tf_idf(word, uniq_id, n)

jobs_words

jobs_words %>%
  select(-total) %>%
  arrange(desc(tf_idf))
```

Don't see value in below graph. Remove

```{r}
jobs_words %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>% 
  group_by(uniq_id) %>% 
  top_n(15) %>% 
  ungroup() %>%
  ggplot(aes(word, tf_idf, fill = uniq_id)) +
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") +
  facet_wrap(~uniq_id, ncol = 2, scales = "free") +
  coord_flip()
```

Use bigrams of 2 to find the true word pairs that occur the most frequently

```{r}
jobs_bigrams <- jobs_df %>%
  unnest_tokens(bigram, job_description, token = "ngrams", n = 2)

jobs_bigrams %>%
  count(bigram, sort = TRUE)

bigrams_separated <- jobs_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

# This result is valuable
bigram_counts %>% print(n = 100)

```

Bigrams with 3 words occurring together to see if there is anything meaningful from that. This appears to be identifying boilerplate text that occurs for many listings but isn't specific to th

```{r}

jobs_df %>%
  unnest_tokens(trigram, job_description, token = "ngrams", n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !word3 %in% stop_words$word) %>%
  count(word1, word2, word3, sort = TRUE)

```

Graph the bigrams

```{r fig.width=10,fig.height=11}
# Visualizing bigrams
library(igraph)

# filter for only relatively common combinations
bigram_graph <- bigram_counts %>%
  filter(n > 10) %>%
  graph_from_data_frame()

bigram_graph
```

Good graph here. Keep

```{r fig.width=10,fig.height=11}
library(ggraph)
set.seed(2020)

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)
```

Good graph here. Keep

```{r fig.width=10,fig.height=11}
set.seed(2021)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()
```

Just a generic plot of the most common words that occur at least 25 times

```{r fig.width=10,fig.height=11}
jobs_words <- jobs_words %>%
    anti_join(stop_words)


jobs_words %>%
  count(word, sort = TRUE)


jobs_words %>%
  count(word, sort = TRUE) %>%
  filter(n > 25) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()

```

# Conclusion

=== Just notes below

# Online searches

## from: https://www.kdnuggets.com/2018/05/simplilearn-9-must-have-skills-data-scientist.html

Education
R programming
python coding
hadoop platform
sql database/coding
apache spark
machine learning and AI
data visualization
unstructured data
intellectual curiosity
business acumen
communication skills
teamwork

## from: https://www.mastersindatascience.org/data-scientist-skills/
communication
business acumen
data-drive problem solving
data visualization
programming
R
python
tableau
hadoop
sql
spark
statistics
mathematics
