---
title: "DATA 607 Project 03 v1"
author: "Philip Tanofsky"
date: "3/21/2020"
output: html_document
---

# Introduction

Attempt to determine the top skills for a data scientist based on job listings for data scientist. Different text mining techniques are used to identify skills from the description section of the job listings.

```{r load-library, echo=TRUE, results='hide', message=FALSE, warning=FALSE}
# Libraries used
library(dplyr)
library(ggplot2)
library(ggraph)
library(igraph)
library(quanteda)
library(tidyr)
library(tidytext)
library(tidyverse)
library(topicmodels)
```

## Process Input Data

Following the NASA case study example presented at https://www.tidytextmining.com/nasa.html for Term Frequency and Topic Modeling.

```{r read-csv}
# Read in CSV file of the 10000 jobs listings for Data Scientist
jobs_df <- read.csv(file = 'data_scientist_united_states_job_postings_jobspikr.csv', stringsAsFactors = FALSE)

# For testing purposes, just the first n rows only
jobs_df <- jobs_df[1:2000,]
```

Job description is the most important field in the dataset for the data mining exercise. The field contains the complete write-up posted for the job listing.

```{r jobs-description}
jobs_desc <- tibble(id = jobs_df$uniq_id, 
                        desc = jobs_df$job_description)

jobs_desc %>% 
  select(desc) %>% 
  sample_n(5)
```

The job category provides some context of the job listing and will be used to capture important words per category.

```{r jobs-category}
jobs_cat <- tibble(id = jobs_df$uniq_id, 
                        category = jobs_df$category)

jobs_cat <- jobs_cat %>% filter(jobs_cat$category != "") %>% print(n=10)
```

From the job_description, tokenize all the words and remove "stop_words" which are common words in the English language to allow for focus on meaningful words of the job listing.

```{r unnest-job-description}
# Use tidytextâ€™s unnest_tokens() for the description field so we can do the text analysis.
# unnest_tokens() will tokenize all the words in the description field and create a tidy dataframe of the word by identifer

jobs_desc <- jobs_desc %>% 
  unnest_tokens(word, desc) %>% 
  anti_join(stop_words)

jobs_desc
```

Provide count in table form of the most common words in the job descriptions.

```{r jobs-description-common-words}
# Most common words in the description field
jobs_desc %>%
  count(word, sort = TRUE) %>% print(n=10)
```

Applying lowercase to all the words to ensure different cases of the same word aren't considered different.

```{r jobs-description-lowercase}
# lowercase all the words just to make sure there's no redundancy
jobs_desc <- jobs_desc %>% 
  mutate(word = tolower(word))
```

## Term Frequency

The term frequency times inverse document frequency (TF-IDF) is used to identify words that are especially important to a document within a collection of documents. The results are the most important words in the description fields as measured by TF-IDF, meaning the words are common but not too common.

1. Calculate the TF-IDF

```{r calculate-tf-idf}
# Calculating tf-idf for the description fields

desc_tf_idf <- jobs_desc %>% 
  count(id, word, sort = TRUE) %>%
  ungroup() %>%
  bind_tf_idf(word, id, n)

desc_tf_idf %>% filter(n >= 10) %>%
  arrange(-tf_idf)
```

2. Combine the data frame of the TF_IDF of the job descriptions with the job categories.

The join is performed on the unique ID as key. Joining with the categories will identify the most important words from the job descriptions per job category.

```{r td-idf-plot, fig.width=10,fig.height=40}
# Join with the category
desc_tf_idf <- full_join(desc_tf_idf, jobs_cat, by = "id")

desc_tf_idf %>% 
  filter(!near(tf, 1)) %>%
  filter(category %in% jobs_cat$category) %>%
  arrange(desc(tf_idf)) %>%
  group_by(category) %>%
  distinct(word, category, .keep_all = TRUE) %>%
  top_n(8, tf_idf) %>% 
  ungroup() %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>%
  ggplot(aes(word, tf_idf, fill = category)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~category, ncol = 3, scales = "free") +
  coord_flip() +
  labs(title = "Highest tf-idf words in job listing description fields",
       caption = "From jobpickr dataset",
       x = NULL, y = "tf-idf")
```

The resulting plot did not prove useful for identifying skills across all the job listings for data scientist. The plot does indicate which words are more common across that specific job category. The results demonstrate that job listings by category are likely posted by the same company or same recruiter and thus the same boilerplate description is often used across many job listings.

## Topic Modeling

In order to peform topic modeling, a document term matrix is required.

1. Calculate the word count by document ID. Each job description is considered a unique document by the job listing's unique ID.

```{r word-counts}
# 8.4 Topic Modeling

# Casting to a document-term matrix
word_counts <- jobs_desc %>%
  count(id, word, sort = TRUE) %>%
  ungroup()

word_counts %>% print(n=10)
```

2. Construct the document-term matrix.

The resulting document-term matrix indicates a high level of sparsity. The non-zero entries do correspond to a certain word appearing in a particular document.

```{r construct-dtm}
# Construct DTM
desc_dtm <- word_counts %>%
  cast_dtm(id, word, n)

desc_dtm
```

3. Calculate the LDA

According to Wikipedia, "In natural language processing, the latent Dirichlet allocation (LDA) is a generative statistical model that allows sets of observations to be explained by unobserved groups that explain why some parts of the data are similar."

```{r calculate-lda}
# Rrunning this model is time intensive
# Define there to be 16 topics.
desc_lda <- LDA(desc_dtm, k = 16, control = list(seed = 1234))
desc_lda
```

4. Tidy the resulting LDA topics.

```{r tidy-lda}
# Interpreting the data model
tidy_lda <- tidy(desc_lda)

tidy_lda
```

5. Identify the top 10 terms for each topic.

```{r top-10-tidy-lda}
# Top 10 Terms
top_terms <- tidy_lda %>%
  group_by(topic) %>%
  top_n(10, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

top_terms
```

6. Plot the top 10 terms for each topic.

Even though the topics are anonymous, only identified by number, the groupings of words show some similarities and differences, but do not necessarily provide much value at this point.

The topic modeling process has identified groupings of terms that we can understand as human readers of these description fields.

```{r top-terms-plot, fig.width=10,fig.height=11}
top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  group_by(topic, term) %>%    
  arrange(desc(beta)) %>%  
  ungroup() %>%
  ggplot(aes(term, beta, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  scale_x_reordered() +
  labs(title = "Top 10 terms in each LDA topic",
       x = NULL, y = expression(beta)) +
  facet_wrap(~ topic, ncol = 4, scales = "free")
```

7. Calculate gamma

Gamma will define the probability that each document belongs in each topic.

```{r lda-gamma, fig.width=10,fig.height=11}
# LDA gamma
lda_gamma <- tidy(desc_lda, matrix = "gamma")

lda_gamma
```

8. Identify the categories associated with each topic

```{r lda-gamma-top-category}
lda_gamma <- full_join(lda_gamma, jobs_cat, by = c("document" = "id"))

lda_gamma

top_cats <- lda_gamma %>% 
  filter(gamma > 0.5) %>% 
  count(topic, category, sort = TRUE)

top_cats <- top_cats %>% filter(!is.na(category))
```

Topic 9 identifes 'business and financial operations' as the top category, and the only topic to include the term 'aws'. Topic 4, most identified with category 'Arts/Entertainment/Publishing' contains the terms 'experience' and 'content' which align with the category broadly.

```{r lda-gamma-top-category-plot, fig.width=10,fig.height=12}
# One more graph from 8.4.4
top_cats %>%
  group_by(topic) %>%
  top_n(5, n) %>%
  ungroup %>%
  mutate(category = reorder_within(category, n, topic)) %>%
  ggplot(aes(category, n, fill = as.factor(topic))) +
  geom_col(show.legend = FALSE) +
  labs(title = "Top categories for each LDA topic",
       x = NULL, y = "Number of documents") +
  coord_flip() +
  scale_x_reordered() +
  facet_wrap(~ topic, ncol = 2, scales = "free")
```

## Lexical Dispersion

Following the example at **https://www.r-bloggers.com/advancing-text-mining-with-r-and-quanteda/** and  **https://quanteda.io/articles/pkgdown/examples/plotting.html**

1. Create a corpus based on the unique ID and the job description.

```{r generate-corpus}
# Generate a corpus

uniq_jobs_df <- jobs_df %>% distinct(uniq_id, .keep_all = TRUE)

my_corpus <- corpus(uniq_jobs_df, docid_field = "uniq_id", text_field = "job_description")

mycorpus_stats <- summary(my_corpus)
```

2. Preprocess the text.

Remove numbers, remove punctuation, remove symbols, remove URLs, split hyphens. Clean for OCR.

```{r preprocess-corpus}
# Preprocess the text

# Create tokens
token <-
  tokens(
    my_corpus,
    remove_numbers  = TRUE,
    remove_punct    = TRUE,
    remove_symbols  = TRUE,
    remove_url      = TRUE,
    split_hyphens   = TRUE
  )

# Clean tokens created by OCR
token_ungd <- tokens_select(
  token,
  c("[\\d-]", "[[:punct:]]", "^.{1,2}$"),
  selection = "remove",
  valuetype = "regex",
  verbose = TRUE
)
```

3. Create a Data Frequency Matrix

Using the Quanteda library, create the data frequency matrix and filter words that appear less than 7.5% and more than 90%.

```{r create-dtm-from-corpus}
# Data frequency matrix
my_dfm <- dfm(token_ungd,
              tolower = TRUE,
              stem = TRUE,
              remove = stopwords("english")
              )

my_dfm_trim <-
  dfm_trim(
    my_dfm,
    min_docfreq = 0.075,
    # min 7.5%
    max_docfreq = 0.90,
    # max 90%
    docfreq_type = "prop"
  )

head(dfm_sort(my_dfm_trim, decreasing = TRUE, margin = "both"),
     n = 10,
     nf = 10)
```

4. Plot lexical dispersion

Plot shows the occurrences of the term 'python' and 'r' in across all documents for the state of Oregon (OR). The state was chosen to give a natural subset of all documents initially included.

The lexical dispersion appears to indicate the use of the terms 'python' and 'r' often occur in conjunction in the documents (job descriptions) which would indicate the listings are listing the two programming languages in or near the same sentence. 

```{r lexical-python-r-plot, fig.width=10,fig.height=11}

my_corpus_sub <- corpus_subset(my_corpus, state == "OR")

theme_set(theme_bw())

g <- textplot_xray(
     kwic(my_corpus_sub, pattern = "python"),
     kwic(my_corpus_sub, pattern = "r")
)

g + aes(color = keyword) + 
    scale_color_manual(values = c("blue", "red")) +
    theme(legend.position = "none")

```

## Bigrams

From the tutorial at **https://www.tidytextmining.com/**. The bigrams identify the word pairs that occur the most frequently

1. Identify bigrams of n=2.

This exercise finds bigrams of two words. The function does allow for bigrams of greater than two.

```{r identify-bigrams}
jobs_bigrams <- jobs_df %>%
  unnest_tokens(bigram, job_description, token = "ngrams", n = 2)

jobs_bigrams %>%
  count(bigram, sort = TRUE)

bigrams_separated <- jobs_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>% 
  count(word1, word2, sort = TRUE)

# This result is valuable
bigram_counts %>% print(n = 20)
```

2. Filter the bigrams

Include bigrams that occurred at least 1250 times in order to filter out visual noise.

```{r filter-bigrams, fig.width=10,fig.height=11}
# filter for only relatively common combinations
bigram_graph <- bigram_counts %>%
  filter(n > 1250) %>%
  graph_from_data_frame()

bigram_graph
```

3. Visualize the bigrams in Network plot

```{r bigrams-plot-part1, fig.width=10,fig.height=11}
set.seed(2020)

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)
```

4. Visualize the bigrams with Directional plot

```{r bigrams-plot-part2, fig.width=10,fig.height=11}
set.seed(2021)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE,
                 arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()
```

## Frequency Plot

Simple bar plot of the most common words across all the job listings' descriptions.

```{r common-words-plot-flip, fig.width=10,fig.height=11}
jobs_words <- jobs_df %>%
  unnest_tokens(word, job_description) %>%
  anti_join(stop_words) %>%
  count(uniq_id, word, sort = TRUE)

total_words <- jobs_words %>% 
  group_by(uniq_id) %>% 
  summarize(total = sum(n))

jobs_words <- left_join(jobs_words, total_words)

jobs_words <- jobs_words %>%
    anti_join(stop_words)

jobs_words %>%
  count(word, sort = TRUE)

jobs_words %>%
  count(word, sort = TRUE) %>%
  filter(n > 2500) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()
```

# Conclusion



